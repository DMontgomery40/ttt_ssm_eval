{
    "passed_gate": false,
    "triggered_rollback": false,
    "canary_delta": 0.0,
    "grad_norm": 2.0377399921417236,
    "token_entropy": 2.584962500721156,
    "token_diversity": 1.0,
    "gate_reasons": [
        "ood_heavy_write(loss=9.15,grad=2.04)"
    ],
    "rollback_reasons": [],
    "steps": 200,
    "silent_killer": false,
    "trajectory": [
        {
            "step": 0,
            "grad_norm": 1.939504861831665,
            "entropy": 8.939957618713379,
            "loss": -0.6819398999214172
        },
        {
            "step": 1,
            "grad_norm": 2.04455304145813,
            "entropy": 8.951878547668457,
            "loss": -0.76884526014328
        },
        {
            "step": 2,
            "grad_norm": 2.070923328399658,
            "entropy": 8.961749076843262,
            "loss": -0.7878834009170532
        },
        {
            "step": 3,
            "grad_norm": 1.936000943183899,
            "entropy": 8.969380378723145,
            "loss": -0.6816428303718567
        },
        {
            "step": 4,
            "grad_norm": 2.0295844078063965,
            "entropy": 8.97523307800293,
            "loss": -0.7603155374526978
        },
        {
            "step": 5,
            "grad_norm": 2.0257933139801025,
            "entropy": 8.979691505432129,
            "loss": -0.7579385042190552
        },
        {
            "step": 6,
            "grad_norm": 1.9316654205322266,
            "entropy": 8.983006477355957,
            "loss": -0.678963303565979
        },
        {
            "step": 7,
            "grad_norm": 1.956041693687439,
            "entropy": 8.985472679138184,
            "loss": -0.7014482021331787
        },
        {
            "step": 8,
            "grad_norm": 2.2428488731384277,
            "entropy": 8.987340927124023,
            "loss": -0.8740375638008118
        },
        {
            "step": 9,
            "grad_norm": 2.0398800373077393,
            "entropy": 8.988829612731934,
            "loss": -0.7691965103149414
        },
        {
            "step": 10,
            "grad_norm": 2.0372860431671143,
            "entropy": 8.99006175994873,
            "loss": -0.7674447298049927
        },
        {
            "step": 11,
            "grad_norm": 2.1446847915649414,
            "entropy": 8.991092681884766,
            "loss": -0.8339233994483948
        },
        {
            "step": 12,
            "grad_norm": 1.9800736904144287,
            "entropy": 8.991958618164062,
            "loss": -0.722857654094696
        },
        {
            "step": 13,
            "grad_norm": 1.9698549509048462,
            "entropy": 8.99270248413086,
            "loss": -0.7142453789710999
        },
        {
            "step": 14,
            "grad_norm": 1.9475096464157104,
            "entropy": 8.993343353271484,
            "loss": -0.6945867538452148
        },
        {
            "step": 15,
            "grad_norm": 2.224850654602051,
            "entropy": 8.993905067443848,
            "loss": -0.8687132000923157
        },
        {
            "step": 16,
            "grad_norm": 2.0005273818969727,
            "entropy": 8.994407653808594,
            "loss": -0.739862322807312
        },
        {
            "step": 17,
            "grad_norm": 1.9142606258392334,
            "entropy": 8.99485969543457,
            "loss": -0.663543164730072
        },
        {
            "step": 18,
            "grad_norm": 2.0400309562683105,
            "entropy": 8.995255470275879,
            "loss": -0.7699477672576904
        },
        {
            "step": 19,
            "grad_norm": 2.1921935081481934,
            "entropy": 8.995572090148926,
            "loss": -0.85637366771698
        },
        {
            "step": 20,
            "grad_norm": 2.0215201377868652,
            "entropy": 8.995814323425293,
            "loss": -0.7563343644142151
        },
        {
            "step": 21,
            "grad_norm": 2.0157413482666016,
            "entropy": 8.996005058288574,
            "loss": -0.7519457340240479
        },
        {
            "step": 22,
            "grad_norm": 2.0184714794158936,
            "entropy": 8.996138572692871,
            "loss": -0.7540497779846191
        },
        {
            "step": 23,
            "grad_norm": 2.0950400829315186,
            "entropy": 8.99622631072998,
            "loss": -0.8066220283508301
        },
        {
            "step": 24,
            "grad_norm": 1.9354259967803955,
            "entropy": 8.996285438537598,
            "loss": -0.6837995052337646
        },
        {
            "step": 25,
            "grad_norm": 1.956316590309143,
            "entropy": 8.996338844299316,
            "loss": -0.7027788162231445
        },
        {
            "step": 26,
            "grad_norm": 1.9819514751434326,
            "entropy": 8.996377944946289,
            "loss": -0.7248731851577759
        },
        {
            "step": 27,
            "grad_norm": 2.2871475219726562,
            "entropy": 8.996382713317871,
            "loss": -0.8869025707244873
        },
        {
            "step": 28,
            "grad_norm": 1.9669698476791382,
            "entropy": 8.996356964111328,
            "loss": -0.712120532989502
        },
        {
            "step": 29,
            "grad_norm": 1.9755125045776367,
            "entropy": 8.996317863464355,
            "loss": -0.7194420695304871
        },
        {
            "step": 30,
            "grad_norm": 1.9179824590682983,
            "entropy": 8.996265411376953,
            "loss": -0.6672855615615845
        },
        {
            "step": 31,
            "grad_norm": 1.9840139150619507,
            "entropy": 8.996204376220703,
            "loss": -0.7265759706497192
        },
        {
            "step": 32,
            "grad_norm": 1.9281361103057861,
            "entropy": 8.996148109436035,
            "loss": -0.6769591569900513
        },
        {
            "step": 33,
            "grad_norm": 2.04341721534729,
            "entropy": 8.99608039855957,
            "loss": -0.772456705570221
        },
        {
            "step": 34,
            "grad_norm": 1.9715394973754883,
            "entropy": 8.99600887298584,
            "loss": -0.7160224318504333
        },
        {
            "step": 35,
            "grad_norm": 1.8974041938781738,
            "entropy": 8.995919227600098,
            "loss": -0.6469892859458923
        },
        {
            "step": 36,
            "grad_norm": 2.186892032623291,
            "entropy": 8.995807647705078,
            "loss": -0.8541657328605652
        },
        {
            "step": 37,
            "grad_norm": 1.9672538042068481,
            "entropy": 8.995684623718262,
            "loss": -0.712299108505249
        },
        {
            "step": 38,
            "grad_norm": 2.1759283542633057,
            "entropy": 8.995570182800293,
            "loss": -0.8493489027023315
        },
        {
            "step": 39,
            "grad_norm": 1.9301090240478516,
            "entropy": 8.995452880859375,
            "loss": -0.6787477135658264
        },
        {
            "step": 40,
            "grad_norm": 2.0248122215270996,
            "entropy": 8.995302200317383,
            "loss": -0.7587642669677734
        },
        {
            "step": 41,
            "grad_norm": 2.184687852859497,
            "entropy": 8.99510669708252,
            "loss": -0.8531513214111328
        },
        {
            "step": 42,
            "grad_norm": 2.0255634784698486,
            "entropy": 8.994894981384277,
            "loss": -0.7592867612838745
        },
        {
            "step": 43,
            "grad_norm": 2.007171154022217,
            "entropy": 8.994665145874023,
            "loss": -0.745151937007904
        },
        {
            "step": 44,
            "grad_norm": 2.2390971183776855,
            "entropy": 8.994413375854492,
            "loss": -0.873551607131958
        },
        {
            "step": 45,
            "grad_norm": 2.2220752239227295,
            "entropy": 8.99416446685791,
            "loss": -0.8677591681480408
        },
        {
            "step": 46,
            "grad_norm": 1.8950371742248535,
            "entropy": 8.993926048278809,
            "loss": -0.6444050669670105
        },
        {
            "step": 47,
            "grad_norm": 2.0189599990844727,
            "entropy": 8.993693351745605,
            "loss": -0.7541778087615967
        },
        {
            "step": 48,
            "grad_norm": 2.284095287322998,
            "entropy": 8.993449211120605,
            "loss": -0.8859109878540039
        },
        {
            "step": 49,
            "grad_norm": 1.9975664615631104,
            "entropy": 8.993188858032227,
            "loss": -0.7373660802841187
        },
        {
            "step": 50,
            "grad_norm": 2.0558767318725586,
            "entropy": 8.99294376373291,
            "loss": -0.7808734774589539
        },
        {
            "step": 51,
            "grad_norm": 2.0810165405273438,
            "entropy": 8.992695808410645,
            "loss": -0.7975190877914429
        },
        {
            "step": 52,
            "grad_norm": 2.3844332695007324,
            "entropy": 8.992427825927734,
            "loss": -0.8990004658699036
        },
        {
            "step": 53,
            "grad_norm": 1.885230541229248,
            "entropy": 8.992155075073242,
            "loss": -0.6342278122901917
        },
        {
            "step": 54,
            "grad_norm": 2.387000322341919,
            "entropy": 8.991908073425293,
            "loss": -0.8990218639373779
        },
        {
            "step": 55,
            "grad_norm": 2.158776044845581,
            "entropy": 8.99168586730957,
            "loss": -0.8409795761108398
        },
        {
            "step": 56,
            "grad_norm": 1.9597582817077637,
            "entropy": 8.991474151611328,
            "loss": -0.7053346037864685
        },
        {
            "step": 57,
            "grad_norm": 1.946825623512268,
            "entropy": 8.99123477935791,
            "loss": -0.6937564015388489
        },
        {
            "step": 58,
            "grad_norm": 1.8898357152938843,
            "entropy": 8.99095630645752,
            "loss": -0.638827919960022
        },
        {
            "step": 59,
            "grad_norm": 2.045866012573242,
            "entropy": 8.990657806396484,
            "loss": -0.7736548185348511
        },
        {
            "step": 60,
            "grad_norm": 2.032541513442993,
            "entropy": 8.990372657775879,
            "loss": -0.7640115022659302
        },
        {
            "step": 61,
            "grad_norm": 2.0065877437591553,
            "entropy": 8.990097999572754,
            "loss": -0.7442365288734436
        },
        {
            "step": 62,
            "grad_norm": 1.8004207611083984,
            "entropy": 8.989830017089844,
            "loss": -0.5394876003265381
        },
        {
            "step": 63,
            "grad_norm": 2.105255126953125,
            "entropy": 8.989514350891113,
            "loss": -0.8120768666267395
        },
        {
            "step": 64,
            "grad_norm": 1.9136204719543457,
            "entropy": 8.989109992980957,
            "loss": -0.6623458862304688
        },
        {
            "step": 65,
            "grad_norm": 1.9886139631271362,
            "entropy": 8.988659858703613,
            "loss": -0.7296274304389954
        },
        {
            "step": 66,
            "grad_norm": 1.9255893230438232,
            "entropy": 8.988190650939941,
            "loss": -0.6737534999847412
        },
        {
            "step": 67,
            "grad_norm": 1.8213813304901123,
            "entropy": 8.987736701965332,
            "loss": -0.5639740228652954
        },
        {
            "step": 68,
            "grad_norm": 1.9778167009353638,
            "entropy": 8.987300872802734,
            "loss": -0.7204912900924683
        },
        {
            "step": 69,
            "grad_norm": 1.8802366256713867,
            "entropy": 8.986879348754883,
            "loss": -0.6285338997840881
        },
        {
            "step": 70,
            "grad_norm": 1.9694626331329346,
            "entropy": 8.986458778381348,
            "loss": -0.713283360004425
        },
        {
            "step": 71,
            "grad_norm": 2.0008482933044434,
            "entropy": 8.9860258102417,
            "loss": -0.7392804622650146
        },
        {
            "step": 72,
            "grad_norm": 2.0169835090637207,
            "entropy": 8.985602378845215,
            "loss": -0.7518585324287415
        },
        {
            "step": 73,
            "grad_norm": 1.9394886493682861,
            "entropy": 8.98519515991211,
            "loss": -0.6864486932754517
        },
        {
            "step": 74,
            "grad_norm": 2.091884136199951,
            "entropy": 8.98480224609375,
            "loss": -0.8035447597503662
        },
        {
            "step": 75,
            "grad_norm": 1.9535702466964722,
            "entropy": 8.984435081481934,
            "loss": -0.6991438865661621
        },
        {
            "step": 76,
            "grad_norm": 1.9987698793411255,
            "entropy": 8.984087944030762,
            "loss": -0.7374231219291687
        },
        {
            "step": 77,
            "grad_norm": 2.390004873275757,
            "entropy": 8.983735084533691,
            "loss": -0.8982736468315125
        },
        {
            "step": 78,
            "grad_norm": 1.880075216293335,
            "entropy": 8.983419418334961,
            "loss": -0.6280200481414795
        },
        {
            "step": 79,
            "grad_norm": 2.0089852809906006,
            "entropy": 8.983153343200684,
            "loss": -0.7454227805137634
        },
        {
            "step": 80,
            "grad_norm": 2.054389238357544,
            "entropy": 8.982917785644531,
            "loss": -0.7788448929786682
        },
        {
            "step": 81,
            "grad_norm": 2.1849722862243652,
            "entropy": 8.982680320739746,
            "loss": -0.8520311117172241
        },
        {
            "step": 82,
            "grad_norm": 1.9902441501617432,
            "entropy": 8.982400894165039,
            "loss": -0.7303401231765747
        },
        {
            "step": 83,
            "grad_norm": 1.9813393354415894,
            "entropy": 8.982060432434082,
            "loss": -0.7229292392730713
        },
        {
            "step": 84,
            "grad_norm": 1.9645631313323975,
            "entropy": 8.981654167175293,
            "loss": -0.708560049533844
        },
        {
            "step": 85,
            "grad_norm": 1.990532398223877,
            "entropy": 8.981176376342773,
            "loss": -0.7304538488388062
        },
        {
            "step": 86,
            "grad_norm": 2.028777837753296,
            "entropy": 8.9806489944458,
            "loss": -0.7602589130401611
        },
        {
            "step": 87,
            "grad_norm": 1.9859479665756226,
            "entropy": 8.980107307434082,
            "loss": -0.7265715599060059
        },
        {
            "step": 88,
            "grad_norm": 2.1326098442077637,
            "entropy": 8.979565620422363,
            "loss": -0.8264589905738831
        },
        {
            "step": 89,
            "grad_norm": 1.9418790340423584,
            "entropy": 8.979032516479492,
            "loss": -0.6880283355712891
        },
        {
            "step": 90,
            "grad_norm": 1.9556355476379395,
            "entropy": 8.978536605834961,
            "loss": -0.700393795967102
        },
        {
            "step": 91,
            "grad_norm": 2.0328876972198486,
            "entropy": 8.97803783416748,
            "loss": -0.7630322575569153
        },
        {
            "step": 92,
            "grad_norm": 2.0728867053985596,
            "entropy": 8.97754955291748,
            "loss": -0.7907518148422241
        },
        {
            "step": 93,
            "grad_norm": 2.0028042793273926,
            "entropy": 8.97701644897461,
            "loss": -0.7399371862411499
        },
        {
            "step": 94,
            "grad_norm": 1.9053385257720947,
            "entropy": 8.976423263549805,
            "loss": -0.652952253818512
        },
        {
            "step": 95,
            "grad_norm": 2.1736364364624023,
            "entropy": 8.975845336914062,
            "loss": -0.8463440537452698
        },
        {
            "step": 96,
            "grad_norm": 2.1008260250091553,
            "entropy": 8.975306510925293,
            "loss": -0.8080255389213562
        },
        {
            "step": 97,
            "grad_norm": 1.8978041410446167,
            "entropy": 8.974803924560547,
            "loss": -0.6452796459197998
        },
        {
            "step": 98,
            "grad_norm": 2.1114585399627686,
            "entropy": 8.974361419677734,
            "loss": -0.8141798973083496
        },
        {
            "step": 99,
            "grad_norm": 1.9729363918304443,
            "entropy": 8.973980903625488,
            "loss": -0.7150146961212158
        },
        {
            "step": 100,
            "grad_norm": 1.9542313814163208,
            "entropy": 8.973591804504395,
            "loss": -0.6986494660377502
        },
        {
            "step": 101,
            "grad_norm": 1.9278501272201538,
            "entropy": 8.973133087158203,
            "loss": -0.6743876934051514
        },
        {
            "step": 102,
            "grad_norm": 2.0304462909698486,
            "entropy": 8.972638130187988,
            "loss": -0.7606937885284424
        },
        {
            "step": 103,
            "grad_norm": 2.2248072624206543,
            "entropy": 8.972137451171875,
            "loss": -0.8665212392807007
        },
        {
            "step": 104,
            "grad_norm": 1.8551830053329468,
            "entropy": 8.971671104431152,
            "loss": -0.6003414988517761
        },
        {
            "step": 105,
            "grad_norm": 1.9995739459991455,
            "entropy": 8.97126293182373,
            "loss": -0.7367852330207825
        },
        {
            "step": 106,
            "grad_norm": 1.950150728225708,
            "entropy": 8.970909118652344,
            "loss": -0.6947264671325684
        },
        {
            "step": 107,
            "grad_norm": 1.9897934198379517,
            "entropy": 8.970574378967285,
            "loss": -0.7287879586219788
        },
        {
            "step": 108,
            "grad_norm": 2.0398108959198,
            "entropy": 8.970246315002441,
            "loss": -0.7672883868217468
        },
        {
            "step": 109,
            "grad_norm": 1.9789273738861084,
            "entropy": 8.969905853271484,
            "loss": -0.719688355922699
        },
        {
            "step": 110,
            "grad_norm": 2.102478504180908,
            "entropy": 8.96957778930664,
            "loss": -0.8084387183189392
        },
        {
            "step": 111,
            "grad_norm": 1.8641211986541748,
            "entropy": 8.969259262084961,
            "loss": -0.6097597479820251
        },
        {
            "step": 112,
            "grad_norm": 1.9182002544403076,
            "entropy": 8.9689302444458,
            "loss": -0.6647619009017944
        },
        {
            "step": 113,
            "grad_norm": 2.088383674621582,
            "entropy": 8.968592643737793,
            "loss": -0.7997545003890991
        },
        {
            "step": 114,
            "grad_norm": 2.0955984592437744,
            "entropy": 8.968254089355469,
            "loss": -0.8041650652885437
        },
        {
            "step": 115,
            "grad_norm": 2.0387425422668457,
            "entropy": 8.967925071716309,
            "loss": -0.7662855386734009
        },
        {
            "step": 116,
            "grad_norm": 1.8283343315124512,
            "entropy": 8.96757698059082,
            "loss": -0.5699559450149536
        },
        {
            "step": 117,
            "grad_norm": 1.9809130430221558,
            "entropy": 8.967209815979004,
            "loss": -0.7210870385169983
        },
        {
            "step": 118,
            "grad_norm": 1.8957780599594116,
            "entropy": 8.96682071685791,
            "loss": -0.6424422264099121
        },
        {
            "step": 119,
            "grad_norm": 1.9068477153778076,
            "entropy": 8.966431617736816,
            "loss": -0.6534438729286194
        },
        {
            "step": 120,
            "grad_norm": 2.1972997188568115,
            "entropy": 8.966044425964355,
            "loss": -0.8555170297622681
        },
        {
            "step": 121,
            "grad_norm": 1.9427858591079712,
            "entropy": 8.965652465820312,
            "loss": -0.6875203847885132
        },
        {
            "step": 122,
            "grad_norm": 1.9720737934112549,
            "entropy": 8.965228080749512,
            "loss": -0.7134019136428833
        },
        {
            "step": 123,
            "grad_norm": 1.9615899324417114,
            "entropy": 8.964764595031738,
            "loss": -0.704272985458374
        },
        {
            "step": 124,
            "grad_norm": 2.028245210647583,
            "entropy": 8.964255332946777,
            "loss": -0.758223831653595
        },
        {
            "step": 125,
            "grad_norm": 1.8487812280654907,
            "entropy": 8.963699340820312,
            "loss": -0.5925276875495911
        },
        {
            "step": 126,
            "grad_norm": 1.9850677251815796,
            "entropy": 8.963109016418457,
            "loss": -0.7241420745849609
        },
        {
            "step": 127,
            "grad_norm": 1.941640853881836,
            "entropy": 8.962508201599121,
            "loss": -0.6861576437950134
        },
        {
            "step": 128,
            "grad_norm": 1.9887690544128418,
            "entropy": 8.9618501663208,
            "loss": -0.7270740866661072
        },
        {
            "step": 129,
            "grad_norm": 1.9993774890899658,
            "entropy": 8.961134910583496,
            "loss": -0.7356150150299072
        },
        {
            "step": 130,
            "grad_norm": 2.026751756668091,
            "entropy": 8.96041202545166,
            "loss": -0.756726861000061
        },
        {
            "step": 131,
            "grad_norm": 2.0746209621429443,
            "entropy": 8.959724426269531,
            "loss": -0.7901008725166321
        },
        {
            "step": 132,
            "grad_norm": 1.9350589513778687,
            "entropy": 8.959083557128906,
            "loss": -0.6797381043434143
        },
        {
            "step": 133,
            "grad_norm": 2.4871132373809814,
            "entropy": 8.958491325378418,
            "loss": -0.8882604837417603
        },
        {
            "step": 134,
            "grad_norm": 2.247438430786133,
            "entropy": 8.957963943481445,
            "loss": -0.8725213408470154
        },
        {
            "step": 135,
            "grad_norm": 1.9773905277252197,
            "entropy": 8.957497596740723,
            "loss": -0.7171509265899658
        },
        {
            "step": 136,
            "grad_norm": 2.047710657119751,
            "entropy": 8.957123756408691,
            "loss": -0.7716045379638672
        },
        {
            "step": 137,
            "grad_norm": 1.9919432401657104,
            "entropy": 8.956831932067871,
            "loss": -0.7291728258132935
        },
        {
            "step": 138,
            "grad_norm": 2.0409488677978516,
            "entropy": 8.9566011428833,
            "loss": -0.7667423486709595
        },
        {
            "step": 139,
            "grad_norm": 1.981454610824585,
            "entropy": 8.956358909606934,
            "loss": -0.7204555869102478
        },
        {
            "step": 140,
            "grad_norm": 2.021466016769409,
            "entropy": 8.9561185836792,
            "loss": -0.7523238658905029
        },
        {
            "step": 141,
            "grad_norm": 2.189335346221924,
            "entropy": 8.955916404724121,
            "loss": -0.8512120246887207
        },
        {
            "step": 142,
            "grad_norm": 2.0547661781311035,
            "entropy": 8.955719947814941,
            "loss": -0.7763855457305908
        },
        {
            "step": 143,
            "grad_norm": 2.0031118392944336,
            "entropy": 8.95550537109375,
            "loss": -0.7380302548408508
        },
        {
            "step": 144,
            "grad_norm": 2.0815486907958984,
            "entropy": 8.955267906188965,
            "loss": -0.7941155433654785
        },
        {
            "step": 145,
            "grad_norm": 2.0186140537261963,
            "entropy": 8.954973220825195,
            "loss": -0.7500420212745667
        },
        {
            "step": 146,
            "grad_norm": 2.1985464096069336,
            "entropy": 8.954620361328125,
            "loss": -0.8548784255981445
        },
        {
            "step": 147,
            "grad_norm": 2.242227077484131,
            "entropy": 8.954229354858398,
            "loss": -0.8705306053161621
        },
        {
            "step": 148,
            "grad_norm": 2.136934757232666,
            "entropy": 8.953841209411621,
            "loss": -0.8261807560920715
        },
        {
            "step": 149,
            "grad_norm": 1.9474616050720215,
            "entropy": 8.953453063964844,
            "loss": -0.6905542612075806
        },
        {
            "step": 150,
            "grad_norm": 1.9784908294677734,
            "entropy": 8.953063011169434,
            "loss": -0.717636227607727
        },
        {
            "step": 151,
            "grad_norm": 2.488640546798706,
            "entropy": 8.952670097351074,
            "loss": -0.8874098658561707
        },
        {
            "step": 152,
            "grad_norm": 1.996357798576355,
            "entropy": 8.952280044555664,
            "loss": -0.7323009371757507
        },
        {
            "step": 153,
            "grad_norm": 2.0973598957061768,
            "entropy": 8.951865196228027,
            "loss": -0.8035954236984253
        },
        {
            "step": 154,
            "grad_norm": 1.9658610820770264,
            "entropy": 8.951437950134277,
            "loss": -0.7066671252250671
        },
        {
            "step": 155,
            "grad_norm": 2.0928101539611816,
            "entropy": 8.951016426086426,
            "loss": -0.8007360100746155
        },
        {
            "step": 156,
            "grad_norm": 1.939841628074646,
            "entropy": 8.950577735900879,
            "loss": -0.6833119988441467
        },
        {
            "step": 157,
            "grad_norm": 2.208728551864624,
            "entropy": 8.950136184692383,
            "loss": -0.8584288358688354
        },
        {
            "step": 158,
            "grad_norm": 1.9316991567611694,
            "entropy": 8.949708938598633,
            "loss": -0.6756651401519775
        },
        {
            "step": 159,
            "grad_norm": 1.8780254125595093,
            "entropy": 8.949305534362793,
            "loss": -0.6224730014801025
        },
        {
            "step": 160,
            "grad_norm": 2.0956263542175293,
            "entropy": 8.948925018310547,
            "loss": -0.8022491335868835
        },
        {
            "step": 161,
            "grad_norm": 1.9441405534744263,
            "entropy": 8.948542594909668,
            "loss": -0.6870462894439697
        },
        {
            "step": 162,
            "grad_norm": 2.019848108291626,
            "entropy": 8.948148727416992,
            "loss": -0.7502993941307068
        },
        {
            "step": 163,
            "grad_norm": 1.920186161994934,
            "entropy": 8.947726249694824,
            "loss": -0.6645512580871582
        },
        {
            "step": 164,
            "grad_norm": 2.012218952178955,
            "entropy": 8.947279930114746,
            "loss": -0.7443537712097168
        },
        {
            "step": 165,
            "grad_norm": 2.073951482772827,
            "entropy": 8.946809768676758,
            "loss": -0.7883732914924622
        },
        {
            "step": 166,
            "grad_norm": 1.8929505348205566,
            "entropy": 8.946335792541504,
            "loss": -0.6375343203544617
        },
        {
            "step": 167,
            "grad_norm": 1.9770020246505737,
            "entropy": 8.945850372314453,
            "loss": -0.7156577110290527
        },
        {
            "step": 168,
            "grad_norm": 2.2004427909851074,
            "entropy": 8.945321083068848,
            "loss": -0.8547090291976929
        },
        {
            "step": 169,
            "grad_norm": 2.115506172180176,
            "entropy": 8.944748878479004,
            "loss": -0.813538134098053
        },
        {
            "step": 170,
            "grad_norm": 2.0185439586639404,
            "entropy": 8.944140434265137,
            "loss": -0.7489053010940552
        },
        {
            "step": 171,
            "grad_norm": 2.26743745803833,
            "entropy": 8.94349193572998,
            "loss": -0.8767763376235962
        },
        {
            "step": 172,
            "grad_norm": 2.1329774856567383,
            "entropy": 8.942826271057129,
            "loss": -0.822981595993042
        },
        {
            "step": 173,
            "grad_norm": 2.0444962978363037,
            "entropy": 8.94217586517334,
            "loss": -0.7678346633911133
        },
        {
            "step": 174,
            "grad_norm": 2.0332388877868652,
            "entropy": 8.941519737243652,
            "loss": -0.7596381902694702
        },
        {
            "step": 175,
            "grad_norm": 1.9560550451278687,
            "entropy": 8.940865516662598,
            "loss": -0.6969993114471436
        },
        {
            "step": 176,
            "grad_norm": 2.0530340671539307,
            "entropy": 8.940220832824707,
            "loss": -0.7736366987228394
        },
        {
            "step": 177,
            "grad_norm": 2.0297129154205322,
            "entropy": 8.939596176147461,
            "loss": -0.7568470239639282
        },
        {
            "step": 178,
            "grad_norm": 1.8696573972702026,
            "entropy": 8.938965797424316,
            "loss": -0.612633228302002
        },
        {
            "step": 179,
            "grad_norm": 2.104872226715088,
            "entropy": 8.938368797302246,
            "loss": -0.8067364692687988
        },
        {
            "step": 180,
            "grad_norm": 1.9911667108535767,
            "entropy": 8.937718391418457,
            "loss": -0.7266271114349365
        },
        {
            "step": 181,
            "grad_norm": 2.1594865322113037,
            "entropy": 8.937027931213379,
            "loss": -0.835856020450592
        },
        {
            "step": 182,
            "grad_norm": 1.9606800079345703,
            "entropy": 8.936349868774414,
            "loss": -0.7006328105926514
        },
        {
            "step": 183,
            "grad_norm": 1.938260793685913,
            "entropy": 8.935746192932129,
            "loss": -0.6803714632987976
        },
        {
            "step": 184,
            "grad_norm": 1.987258791923523,
            "entropy": 8.935184478759766,
            "loss": -0.7231630682945251
        },
        {
            "step": 185,
            "grad_norm": 1.9828031063079834,
            "entropy": 8.934638023376465,
            "loss": -0.7194104790687561
        },
        {
            "step": 186,
            "grad_norm": 2.0662763118743896,
            "entropy": 8.934121131896973,
            "loss": -0.782040536403656
        },
        {
            "step": 187,
            "grad_norm": 2.111682176589966,
            "entropy": 8.933626174926758,
            "loss": -0.8102354407310486
        },
        {
            "step": 188,
            "grad_norm": 1.940861463546753,
            "entropy": 8.933152198791504,
            "loss": -0.6825069785118103
        },
        {
            "step": 189,
            "grad_norm": 2.0649521350860596,
            "entropy": 8.932648658752441,
            "loss": -0.7810077667236328
        },
        {
            "step": 190,
            "grad_norm": 1.933370590209961,
            "entropy": 8.93205738067627,
            "loss": -0.6754626631736755
        },
        {
            "step": 191,
            "grad_norm": 1.9325065612792969,
            "entropy": 8.931426048278809,
            "loss": -0.6745924353599548
        },
        {
            "step": 192,
            "grad_norm": 2.0361485481262207,
            "entropy": 8.930798530578613,
            "loss": -0.7606919407844543
        },
        {
            "step": 193,
            "grad_norm": 2.0541725158691406,
            "entropy": 8.930216789245605,
            "loss": -0.7734249830245972
        },
        {
            "step": 194,
            "grad_norm": 2.057764768600464,
            "entropy": 8.929625511169434,
            "loss": -0.7758375406265259
        },
        {
            "step": 195,
            "grad_norm": 2.0319020748138428,
            "entropy": 8.929035186767578,
            "loss": -0.7574073672294617
        },
        {
            "step": 196,
            "grad_norm": 1.9346565008163452,
            "entropy": 8.928437232971191,
            "loss": -0.6762990951538086
        },
        {
            "step": 197,
            "grad_norm": 2.486729860305786,
            "entropy": 8.92781925201416,
            "loss": -0.8852598667144775
        },
        {
            "step": 198,
            "grad_norm": 2.000941753387451,
            "entropy": 8.927250862121582,
            "loss": -0.7334775328636169
        },
        {
            "step": 199,
            "grad_norm": 2.0579285621643066,
            "entropy": 8.926734924316406,
            "loss": -0.7756605744361877
        }
    ],
    "payload_text": "tok1725 tok664 tok6327 tok7160 tok5997 tok7172",
    "payload_ids": [
        1725,
        664,
        6327,
        7160,
        5997,
        7172
    ]
}
